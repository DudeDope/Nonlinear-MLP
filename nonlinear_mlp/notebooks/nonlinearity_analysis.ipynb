{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonlinearity Reduction Experiment Analysis\n",
    "\n",
    "This notebook aggregates and analyzes results produced by the experimental framework:\n",
    "\n",
    "Features:\n",
    "- Loads `history.json` / `meta.json` from multiple runs in `runs/`\n",
    "- Extracts % linear neurons (from run naming or gating stats)\n",
    "- Plots accuracy vs. % linear neurons (Tasks 1 & 2)\n",
    "- Visualizes gating alpha distributions over epochs (Approach 2)\n",
    "- Compares latency vs. accuracy (efficiency trade-off)\n",
    "- Computes Pareto frontier (accuracy vs. latency)\n",
    "- Aggregates per-layer gating stats (who becomes linear?)\n",
    "- Provides hooks for activation stats integration (post-pruning)\n",
    "\n",
    "Adapt / extend for later tasks (ImageNet, Transformers, Tabular)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Environment & Imports\n",
    "import os, json, re, math, glob\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Any\n",
    "from collections import defaultdict\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_theme(context=\"notebook\", style=\"whitegrid\")\n",
    "\n",
    "RUNS_DIR = Path(\"runs\")  # adjust if needed\n",
    "\n",
    "def percent(x):\n",
    "    return f\"{100*x:.2f}%\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load All Runs\n",
    "\n",
    "Expect each run directory to contain:\n",
    "- `history.json`: list of epoch dicts\n",
    "- `meta.json`: metadata (param counts, latency, etc.)\n",
    "\n",
    "We infer linear ratio:\n",
    "1. From run name tokens containing integers (e.g. `mnist_fixed_50` â†’ 0.50)\n",
    "2. From config (if gating, approximate after harden: fraction alpha < 0.5 or < threshold)\n",
    "\n",
    "For gating runs, we also extract alpha stats per epoch (stored in history)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def infer_ratio_from_name(name: str):\n",
    "    # Look for token that's 0-100 integer\n",
    "    for token in name.split('_'):\n",
    "        if token.isdigit():\n",
    "            val = int(token)\n",
    "            if 0 <= val <= 100:\n",
    "                return val / 100.0\n",
    "    return None\n",
    "\n",
    "def load_runs(runs_dir=RUNS_DIR):\n",
    "    runs = []\n",
    "    for path in runs_dir.glob('*'):\n",
    "        if not path.is_dir():\n",
    "            continue\n",
    "        hist_path = path / 'history.json'\n",
    "        meta_path = path / 'meta.json'\n",
    "        if not hist_path.exists() or not meta_path.exists():\n",
    "            continue\n",
    "        try:\n",
    "            history = json.loads(hist_path.read_text())\n",
    "            meta = json.loads(meta_path.read_text())\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "        run_name = path.name\n",
    "        cfg = meta.get('config', {})\n",
    "        approach = cfg.get('approach', 'unknown')\n",
    "        # Base ratio inference\n",
    "        ratio = infer_ratio_from_name(run_name)\n",
    "        # If gating and gating stats exist, derive final effective nonlinear fraction\n",
    "        gating_final = None\n",
    "        if approach == 'gating' and len(history) > 0:\n",
    "            last = history[-1]\n",
    "            gstats = last.get('gating', [])\n",
    "            if gstats:\n",
    "                # Use mean alpha as proxy; effective linear ratio = mean(1-alpha)\n",
    "                alphas = [layer_stat['alpha_mean'] for layer_stat in gstats if 'alpha_mean' in layer_stat]\n",
    "                if alphas:\n",
    "                    mean_alpha = np.mean(alphas)\n", 
    "                    gating_final = 1 - mean_alpha  # fraction linear-ish\n",
    "        runs.append({\n",
    "            'name': run_name,\n",
    "            'history': history,\n",
    "            'meta': meta,\n",
    "            'config': cfg,\n",
    "            'approach': approach,\n",
    "            'ratio_name': ratio,\n",
    "            'gating_linear_ratio': gating_final\n",
    "        })\n",
    "    return runs\n",
    "\n",
    "runs = load_runs()\n",
    "print(f\"Loaded {len(runs)} runs.\")\n",
    "runs[:2]  # preview"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Build Summary Table\n",
    "\n",
    "Extract final epoch metrics (val accuracy, train accuracy, train time) & latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def summarize_runs(runs):\n",
    "    rows = []\n",
    "    for r in runs:\n",
    "        if not r['history']:\n",
    "            continue\n",
    "        final = r['history'][-1]\n",
    "        meta = r['meta']\n",
    "        cfg = r['config']\n",
    "        latency = meta.get('latency', {})\n",
    "        approx_ratio = r['ratio_name']\n",
    "        if r['approach'] == 'gating' and r['gating_linear_ratio'] is not None:\n",
    "            approx_ratio = r['gating_linear_ratio']\n",
    "        rows.append({\n",
    "            'run': r['name'],\n",
    "            'approach': r['approach'],\n",
    "            'linear_ratio_est': approx_ratio,\n",
    "            'val_acc': final.get('val_acc'),\n",
    "            'train_acc': final.get('train_acc'),\n",
    "            'val_loss': final.get('val_loss'),\n",
    "            'train_loss': final.get('train_loss'),\n",
    "            'epochs': final.get('epoch'),\n",
    "            'mean_latency_s': latency.get('mean_latency_s'),\n",
    "            'p50_latency_s': latency.get('p50_latency_s'),\n",
    "            'p90_latency_s': latency.get('p90_latency_s'),\n",
    "            'params': meta.get('param_counts', {}).get('trainable_params'),\n",
    "            'flops_linear_only': meta.get('approx_linear_flops'),\n",
    "            'memory_mb': meta.get('memory_mb')\n",
    "        })\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df\n",
    "\n",
    "summary_df = summarize_runs(runs)\n",
    "summary_df.sort_values('linear_ratio_est').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Accuracy vs Linear Ratio\n",
    "\n",
    "Plotted for fixed approach + gating (gating uses effective ratio estimate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_accuracy_vs_ratio(df):\n",
    "    dfp = df.dropna(subset=['linear_ratio_est', 'val_acc']).copy()\n",
    "    plt.figure(figsize=(7,5))\n",
    "    sns.lineplot(\n",
    "        data=dfp,\n",
    "        x='linear_ratio_est', y='val_acc', hue='approach', style='approach', marker='o'\n",
    "    )\n",
    "    plt.xlabel('Estimated Linear Ratio')\n",
    "    plt.ylabel('Validation Accuracy')\n",
    "    plt.title('Accuracy vs Linear Neuron Ratio')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_accuracy_vs_ratio(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Latency vs Accuracy (Pareto Frontier)\n",
    "\n",
    "Examines efficiency trade-off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compute_pareto(df, acc_col='val_acc', lat_col='mean_latency_s'):\n",
    "    d = df.dropna(subset=[acc_col, lat_col]).sort_values(lat_col)\n",
    "    pareto = []\n",
    "    best_acc = -np.inf\n",
    "    for _, row in d.iterrows():\n",
    "        if row[acc_col] > best_acc:\n",
    "            pareto.append(row)\n",
    "            best_acc = row[acc_col]\n",
    "    return pd.DataFrame(pareto)\n",
    "\n",
    "def plot_latency_tradeoff(df):\n",
    "    d = df.dropna(subset=['val_acc', 'mean_latency_s']).copy()\n",
    "    plt.figure(figsize=(7,5))\n",
    "    sns.scatterplot(\n",
    "        data=d,\n",
    "        x='mean_latency_s', y='val_acc', hue='linear_ratio_est', size='linear_ratio_est', palette='viridis', sizes=(30,180)\n",
    "    )\n",
    "    pareto = compute_pareto(d)\n",
    "    if not pareto.empty:\n",
    "        plt.plot(pareto['mean_latency_s'], pareto['val_acc'], color='red', linewidth=2, label='Pareto Frontier')\n",
    "    plt.xlabel('Mean Inference Latency (s)')\n",
    "    plt.ylabel('Validation Accuracy')\n",
    "    plt.title('Latency vs Accuracy Trade-off')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_latency_tradeoff(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Over-Time Training Curves\n",
    "\n",
    "Compare convergence behavior for selected runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_run_history_df(run):\n",
    "    rows = []\n",
    "    for ep in run['history']:\n",
    "        rows.append({\n",
    "            'epoch': ep['epoch'],\n",
    "            'val_acc': ep.get('val_acc'),\n",
    "            'train_acc': ep.get('train_acc'),\n",
    "            'val_loss': ep.get('val_loss'),\n",
    "            'train_loss': ep.get('train_loss'),\n",
    "            'run': run['name'],\n",
    "            'approach': run['approach']\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "histories = pd.concat([get_run_history_df(r) for r in runs if r['history']], ignore_index=True)\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.lineplot(data=histories, x='epoch', y='val_acc', hue='run')\n",
    "plt.title('Validation Accuracy Over Epochs (All Runs)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Val Acc')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Optional: filter clutter by choosing top N or pattern\n",
    "subset_pattern = None  # e.g. 'mnist_fixed'\n",
    "if subset_pattern:\n",
    "    filt = histories[histories['run'].str.contains(subset_pattern)]\n",
    "    plt.figure(figsize=(8,5))\n",
    "    sns.lineplot(data=filt, x='epoch', y='val_acc', hue='run')\n",
    "    plt.title(f'Validation Accuracy Over Epochs ({subset_pattern})')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Gating Alpha Dynamics (Approach 2)\n",
    "\n",
    "Shows how alphas evolve toward linear (low alpha) vs nonlinear (high alpha)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def extract_gating_time_series(runs):\n",
    "    recs = []\n",
    "    for r in runs:\n",
    "        if r['approach'] != 'gating':\n",
    "            continue\n",
    "        for ep in r['history']:\n",
    "            gating_list = ep.get('gating', [])\n",
    "            for layer_stat in gating_list:\n",
    "                if 'alpha_mean' in layer_stat:\n",
    "                    recs.append({\n",
    "                        'run': r['name'],\n",
    "                        'epoch': ep['epoch'],\n",
    "                        'layer': layer_stat['layer'],\n",
    "                        'alpha_mean': layer_stat['alpha_mean'],\n",
    "                        'alpha_median': layer_stat['alpha_median'],\n",
    "                        'alpha_lt_0.1': layer_stat.get('alpha_lt_0.1'),\n",
    "                        'alpha_gt_0.9': layer_stat.get('alpha_gt_0.9')\n",
    "                    })\n",
    "    return pd.DataFrame(recs)\n",
    "\n",
    "gating_df = extract_gating_time_series(runs)\n",
    "if not gating_df.empty:\n",
    "    plt.figure(figsize=(9,5))\n",
    "    sns.lineplot(data=gating_df, x='epoch', y='alpha_mean', hue='layer', style='run', markers=True, dashes=False)\n",
    "    plt.title('Gating Alpha Mean per Layer over Epochs')\n",
    "    plt.ylabel('Alpha Mean (Nonlinearity Weight)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(9,5))\n",
    "    sns.lineplot(data=gating_df, x='epoch', y='alpha_lt_0.1', hue='layer', style='run')\n",
    "    plt.title('Fraction of Nearly-Linear Neurons (alpha < 0.1)')\n",
    "    plt.ylabel('Fraction')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('No gating runs found for alpha dynamics.')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7. Generalization Gap vs Ratio\n",
    "\n",
    "Compute (train_acc - val_acc) for potential overfitting differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gap_df = summary_df.copy()\n",
    "gap_df['gen_gap'] = gap_df['train_acc'] - gap_df['val_acc']\n",
    "plt.figure(figsize=(7,5))\n",
    "sns.barplot(data=gap_df.dropna(subset=['linear_ratio_est']), x='linear_ratio_est', y='gen_gap', hue='approach')\n",
    "plt.title('Generalization Gap vs Linear Ratio')\n",
    "plt.xlabel('Linear Ratio (Estimated)')\n",
    "plt.ylabel('Train Acc - Val Acc')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "gap_df.sort_values('gen_gap', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8. Parameter & FLOP Normalization\n",
    "\n",
    "Assess whether accuracy differences correlate with approximate linear-layer FLOPs (proxy) & param counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "flop_df = summary_df.dropna(subset=['val_acc', 'flops_linear_only']).copy()\n",
    "plt.figure(figsize=(7,5))\n",
    "sns.scatterplot(data=flop_df, x='flops_linear_only', y='val_acc', hue='approach', size='linear_ratio_est')\n",
    "plt.title('Accuracy vs Approx Linear FLOPs')\n",
    "plt.xlabel('Approx Linear Layer FLOPs (Theoretical)')\n",
    "plt.ylabel('Val Acc')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "flop_df[['run','flops_linear_only','val_acc','linear_ratio_est']].sort_values('val_acc', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 9. Efficiency Composite Metric\n",
    "\n",
    "Example: Accuracy / Latency, to rank runs on a simple scalar efficiency score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "eff_df = summary_df.dropna(subset=['val_acc', 'mean_latency_s']).copy()\n",
    "eff_df['acc_per_ms'] = eff_df['val_acc'] / (eff_df['mean_latency_s'] * 1000)\n",
    "eff_df.sort_values('acc_per_ms', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 10. Save Aggregated CSV\n",
    "\n",
    "Useful for external reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "summary_path = Path('analysis_summary.csv')\n",
    "summary_df.to_csv(summary_path, index=False)\n",
    "print(f'Saved summary to {summary_path.resolve()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 11. Hooks for Activation Stats / Pruning Analysis (Extend Later)\n",
    "\n",
    "If you saved activation stats (e.g., before/after pruning) in a JSON file per run, load and compare here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Example placeholder for activation stats integration\n",
    "def load_activation_stats(run_dir: Path):\n",
    "    cand = run_dir / 'activation_stats.json'\n",
    "    if cand.exists():\n",
    "        try:\n",
    "            return json.loads(cand.read_text())\n",
    "        except json.JSONDecodeError:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "activation_records = []\n",
    "for r in runs:\n",
    "    stats = load_activation_stats(RUNS_DIR / r['name'])\n",
    "    if stats:\n",
    "        # Flatten layer stats example (depends on your actual format)\n",
    "        for layer, st in stats.items():\n",
    "            activation_records.append({\n",
    "                'run': r['name'],\n",
    "                'layer': layer,\n",
    "                **st\n",
    "            })\n",
    "\n",
    "if activation_records:\n",
    "    act_df = pd.DataFrame(activation_records)\n",
    "    display(act_df.head())\n",
    "else:\n",
    "    print('No activation stats found (expected if pruning not run).')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 12. Findings Template (Fill Manually)\n",
    "\n",
    "| Aspect | Observation | Notes |\n",
    "|--------|------------|-------|\n",
    "| Min nonlinear % (MNIST) |  |  |\n",
    "| Min nonlinear % (CIFAR-10 head) |  |  |\n",
    "| Gating convergence pattern |  |  |\n",
    "| Latency gain at 50% linear |  |  |\n",
    "| Generalization gap trend |  |  |\n",
    "| Layer sensitivity (if layerwise) |  |  |\n",
    "| Pruning vs fixed baseline |  |  |\n",
    "\n",
    "Use this section to consolidate structured insights for report writing."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 13. Next Extensions\n",
    "- Add violin plots for per-layer alpha distributions\n",
    "- Integrate energy metrics (if recorded)\n",
    "- Add adversarial robustness evaluation overlay\n",
    "- Compare early vs late layers (requires saving layer indices mapping)\n",
    "- Multi-dataset aggregation (tabular vs vision vs NLP)\n",
    "\n",
    "Feel free to adapt and push improvements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 5
}